{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rdoEdoa/EESAM/blob/main/RegressionAndRegularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7ntUbR2hEmD"
      },
      "source": [
        "# Linear and Polynomial Regression and regularization\n",
        "As we saw in our lessons, linear regression is a good model only when there is actually a linear dependence from the features to the target.\n",
        "\n",
        "In some cases, for example when the target is a sinusoidal function of the features, a first degree polynomial is not a sufficiently good approximation. In this tutorial we will learn how to create a good polynomial model and how to tune it with regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fFkV4ZQz22v"
      },
      "source": [
        "## Importing packages and functions\n",
        "- [numpy](https://numpy.org) is a python package that provides support for more efficient numerical computation\n",
        "- [pandas](https://pandas.pydata.org) is a convenient library that supports dataframes. Pandas is technically optional because [Scikit-Learn](https://scikit-learn.org) can handle numerical matrices directly, but it will make our lives easier, as we will see later.\n",
        "- [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) is a class that implements the Linear Regression model (all classes in scikit-learn have capital names)\n",
        "- [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) implements the Ridge regularization method for Linear Regression, that adds to the loss function the sum of the squares of the weigths (i.e. minimizes the L2 norm of the vector of weights).\n",
        "- [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) implements the Lasso regularization method for Linear Regression, that adds to the loss function the sum of the modules of the weigths (i.e. minimizes the L1 norm of the vector of weights).\n",
        "- [mean_squared_error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) is the usual Mean Square Error loss function used for Linear Regression (all functions in scikit-learn have lower case names).\n",
        "- [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) splits the dataset into a training set and a validation set (the functionality is the same for test and validation sets, hence the name; here we will use it for the validation set only).\n",
        "- [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) creates various powers of the input features and theor cross-products, as a mapping function for Polynomial Regression\n",
        "- [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) is a utility that can be used to concatenate various ML steps into a single pipeline.\n",
        "- [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) scales your data based on mean and standard deviation, so that it fits in a small interval (e.g. [-1...1]) centered around 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd5YyLV-oI1d"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-hvlPRmiuBa"
      },
      "source": [
        "##  Importing data\n",
        "Let us now read the data for this lab. They cover the Boston housing market (the original is [here](https://www.kaggle.com/c/boston-housing)).\n",
        "\n",
        "As the data is stored in a .csv file, we use the `read_csv()` function from pandas to read it in, show the data format, and print the first three lines.\n",
        "\n",
        "A \"[dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html)\" in pandas (and in other ML frameworks such as R) is a class that includes\n",
        "- a data set (in this case 14 columns and 506 lines)\n",
        "- some format information, such as the column names (from the xlsx header), the data types and so on\n",
        "- methods to print mean, standard deviation and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hrza5EiihRgm"
      },
      "source": [
        "train_df = pd.read_csv('https://raw.githubusercontent.com/lavagno/eesam/main/housing.csv')\n",
        "print(\"Some generic information about the dataframe:\")\n",
        "print(train_df.info())\n",
        "print(\"The first three rows of the dataframe:\")\n",
        "print(train_df.head(3))\n",
        "print(\"The mean of each column:\")\n",
        "print(train_df.mean())\n",
        "print(\"The standard deviation of each column:\")\n",
        "print(train_df.std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mK3QyxOgzqr"
      },
      "source": [
        "The meanings of the data columns are:\n",
        "- CRIM per capita crime rate by town\n",
        "- ZN proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "- INDUS proportion of non-retail business acres per town\n",
        "- CHAS Charles River = 1 if tract bounds river; 0 otherwise\n",
        "- NOX nitric oxides concentration (parts per 10 million)\n",
        "- RM average number of rooms per dwelling\n",
        "- AGE proportion of owner-occupied units built prior to 1940\n",
        "- DIS weighted distances to five Boston employment centres\n",
        "- RAD index of accessibility to radial highways\n",
        "- TAX full-value property-tax rate per 10,000 dollars\n",
        "- PTRATIO pupil-teacher ratio by town\n",
        "- B is a function of the proportion of blacks by town\n",
        "- LSTAT % lower status of the population\n",
        "- MEDV Median value of owner-occupied homes in 1000's of dollars, and this is the **target** of the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxOb69GAGsHs"
      },
      "source": [
        "## Training Linear Regression\n",
        "Let us split our data into a training set and a validation set. We will hold out 30% of the data for validation. We will use a **fixed initial random state** to make our experiment reproducible (in practice, you should never do that, except for model debugging).\n",
        "\n",
        "The `drop()` method of the dataframe simply removes the named column, and it is much easier to use than listing explicitly all the features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yWRd9EOzYHt"
      },
      "source": [
        "X = train_df.drop('MEDV', axis=1)\n",
        "y = train_df['MEDV']\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42, test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFdQiBXxzvHl"
      },
      "source": [
        "Let us establish a baseline by training a linear regression model.\n",
        "\n",
        "The `fit()` function of the `LinearRegression` class performs the training, and the `predict()` function evaluates the model. This is then used to compute our usual loss function (mean square error) and its root (which is directly comparable with the mean house price) for both testing and validation.\n",
        "\n",
        "Finally we also print the [$R^2$ score of the prediction](https://en.wikipedia.org/wiki/Coefficient_of_determination), which is a standard quality index used in statistics. It is similar to the training accuracy measure and it roughly tells how much of the data variance is predicted by the model. It is a number from 0 to 1, and the closer it is to 1, the better our model is.\n",
        "- **Low values of $R^2$ for both sets** are an indication of **underfitting**.\n",
        "- **Lower values of $R^2$ for the validation set than for the training set** are an indication of **overfitting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHhruNlq0OAC"
      },
      "source": [
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_train = lr_model.predict(X_train)\n",
        "mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "rmse_train = math.sqrt(mse_train)\n",
        "print('Training MSE: ', mse_train, ' RMSE: ', rmse_train)\n",
        "\n",
        "y_pred_val = lr_model.predict(X_val)\n",
        "mse_val = mean_squared_error(y_val, y_pred_val)\n",
        "rmse_val = math.sqrt(mse_val)\n",
        "print('Validation MSE: ', mse_val, ' RMSE: ', rmse_val)\n",
        "\n",
        "print('Training score: {}'.format(lr_model.score(X_train, y_train)))\n",
        "print('Validation score: {}'.format(lr_model.score(X_val, y_val)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZPBdLAo3U6C"
      },
      "source": [
        "As you can see, MSEs, RMSEs and scores are not too far from each other. This means that the model is **not grossly overfitting**.\n",
        "\n",
        "On the other hand\n",
        "- both RMSEs, around 4.5, are relatively large with respect to the mean house value (MEDV) of 22, and they are not too far from the standard deviation, which is 9.\n",
        "- both scores are around 0.7, which is low.\n",
        "This means that the model is **probably underfitting**.\n",
        "\n",
        "The only way to \"prove\" and **solve** underfitting, is to try a more complex model, with more parameters, and hence better able to model the underlying data population.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJNfy0o30vCl"
      },
      "source": [
        "## Polynomial Regression\n",
        "We will use **Polynomial Regression**, i.e. linear regression with a set of polynomial mapping functions (in general **Generalized Linear Regression** can use functions of the features such as sine, cosine, exponential, etc.).\n",
        "Thankfully, scikit-learn has an implementation for this and we do not need to do it manually. It is the [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) class.\n",
        "\n",
        "Note that in our lessons we saw polynomial features only of one feature. When there are multiple features, **also the cross-products**, up to the specified degree, will be inserted. For example, if there are two features `[a,b]` the resulting polynomial feature vector will be:\n",
        "```\n",
        "[1, a, b, a^2, ab, b^2]\n",
        "```\n",
        "Since we have 13 features in our dataset, we have 13 features for the 1st degree, 13 for the 2nd degree, and 13*2/2 for the mixed products, plus the constant term, i.e. 105 features and weights in total."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKIPbmmt1LKd"
      },
      "source": [
        "## Scaling\n",
        "Something else that we would like to do is **scale** our data to a range between 0 and 1. This serves the purpose of letting us work with **reasonable numbers when we raise a feature to a power**. As we saw, it also improves the convergence speed for the model, albeit this not really a problem in this case, since the dataset is really small.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMpuZg9f1QJl"
      },
      "source": [
        "## Pipelining\n",
        "Finally, because we need to carry out the same operations on our training and validation (and in real practice test) sets, we will introduce a **pipeline** (conceptually similar to a unix pipeline between processes, or a processor pipeline). This will let us pipe the work done by the methods of our classes, so that the same steps get carried out repeatedly for every variation of the hyperparameters that we want to tune.\n",
        "This is one of the cases in which using an object-oriented language greatly helps, since if all the stages of a pipeline derive from a few parent classes, the pipeline can be designed to work by calling the \"right\" method of each stage.\n",
        "\n",
        "To summarize, we will scale our data, then create polynomial features up to the second degree, and then train a linear regression model.\n",
        "\n",
        "Very conveniently, the pipeline class itself **inherits** the fit and predict methods from the LinearRegression class that it uses, hence we will be able to call them directly from the pipeline, rather than having to access the underlying model instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7aYVieY41AD"
      },
      "source": [
        "steps = [\n",
        "    ('scalar', StandardScaler()),\n",
        "    ('poly', PolynomialFeatures(degree=2)),\n",
        "    ('model', LinearRegression())\n",
        "]\n",
        "\n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred_train = pipeline.predict(X_train)\n",
        "mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "rmse_train = math.sqrt(mse_train)\n",
        "print('Training MSE: ', mse_train, ' RMSE: ', rmse_train)\n",
        "\n",
        "y_pred_val = pipeline.predict(X_val)\n",
        "mse_val = mean_squared_error(y_val, y_pred_val)\n",
        "rmse_val = math.sqrt(mse_val)\n",
        "print('Validation MSE: ', mse_val, ' RMSE: ', rmse_val)\n",
        "\n",
        "print('Training score: {}'.format(pipeline.score(X_train, y_train)))\n",
        "print('Validation score: {}'.format(pipeline.score(X_val, y_val)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH5hcO9E9fga"
      },
      "source": [
        "## Analyzing the polynomial regression results and regularization\n",
        "As expected, the training MSE and RMSE decrease significantly, and the training score increases significantly because the model, being more complex, can better fit the **training** data (but, as we saw, may overfit).\n",
        "\n",
        "But the validation errors and the validation score become much worse. This means that we are now **overfitting**, and now is the time to use **regularization** to reduce its impact.\n",
        "\n",
        "Remember that **some initial overfitting is good**, because it allows us to use regularization to **automatically search for the best model**.\n",
        "\n",
        "We will use Ridge regularization for Linear Regression (also called simply Ridge Regression), which adds the squares of the weights to the loss function.\n",
        "\n",
        "Note that the regularization strength, i.e. the coefficient used to multiply the sum of the squares of the weights, that we called $\\lambda$ in our lessons, is called `alpha` in scikit-learn.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6rtyhDJ_XPE"
      },
      "source": [
        "ridge_steps = [\n",
        "    ('scalar', StandardScaler()),\n",
        "    ('poly', PolynomialFeatures(degree=2)),\n",
        "    ('model', Lasso(alpha=0.1))\n",
        "]\n",
        "\n",
        "ridge_pipe = Pipeline(ridge_steps)\n",
        "ridge_pipe.fit(X_train, y_train)\n",
        "\n",
        "y_pred_train = ridge_pipe.predict(X_train)\n",
        "mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "rmse_train = math.sqrt(mse_train)\n",
        "print('Training MSE: ', mse_train, ' RMSE: ', rmse_train)\n",
        "\n",
        "y_pred_val = ridge_pipe.predict(X_val)\n",
        "mse_val = mean_squared_error(y_val, y_pred_val)\n",
        "rmse_val = math.sqrt(mse_val)\n",
        "print('Validation MSE: ', mse_val, ' RMSE: ', rmse_val)\n",
        "\n",
        "print('Training Score: {}'.format(ridge_pipe.score(X_train, y_train)))\n",
        "print('Validation Score: {}'.format(ridge_pipe.score(X_val, y_val)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-mkzVCH_07w"
      },
      "source": [
        "Now all the measures are **better than with only linear regression**. This means that the target is sensitive to the square or the cross-product of **some features**, but possibly not all of them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So0tRddXOUhN"
      },
      "source": [
        "## Analyzing the results of regularization\n",
        "Let us now print the weights with these two commands.\n",
        "- The first one \"extracts\" the model step from the pipeline.\n",
        "- The second one prints the 105 weights.\n",
        "\n",
        "Note how almost none of the weights are close to zero. If we want to bring them closer to zero, we must:\n",
        "- either increase `alpha` as long as the validation error does not increase \"too much\",\n",
        "- or use another regularization method (the Lasso Regularization mentioned below) whose goal is actually to **force** some weights to become zero.\n",
        "\n",
        "On the other hand, the goal of Ridge is simply to **improve the generalization power of the model**, based on the empirical observation that large weights \"hurt\" generalization, without really forcing them to zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eupIl1XRAv37"
      },
      "source": [
        "lr=ridge_pipe.named_steps['model']\n",
        "print(lr.coef_.size)\n",
        "print(lr.coef_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnPwTg1YUGzd"
      },
      "source": [
        "## Your tasks\n",
        "1. Find (by trial and error) the \"best\" value of `alpha` that leads to the smallest validation error.\n",
        "1. Try another form of regularization, namely the Lasso, which adds the modules of the weights (i.e. minimizes the L1 norm of the vector of weights), rather than their squares. See if:\n",
        " - it improves the validation performance,\n",
        " - it makes some weights closer to zero (with Ridge Regression, the smallest one was $-4\\cdot10^{-4}$).\n",
        "\n",
        "Note that\n",
        "- the `alpha` coefficient for `Ridge` can be arbitrary but must be non-negative (values that are too large or too small may be sub-optimal, of course).\n",
        "- the `alpha` coefficient for Lasso must be between 0 and 1, otherwise the method may not converge to a reasonable model.\n",
        "- with `alpha` (i.e. $\\lambda$) =0 both Ridge and Lasso are simply Linear Regression, of course.\n",
        "- there is a third regularization mechanism, namely [`ElasticNet`](scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) that combines the \"best\" features of both `Lasso` and `Ridge`, using two separate coefficients to minimize both the L1 and L2 norms of the weights. In this particular case, it does not improve things dramatically, but you can try it out if you are curious (as you should be :-) )."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN7JpGejXZ-T"
      },
      "source": [
        "# insert your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjuBEq5gY84f"
      },
      "source": [
        "## Analysis hints\n",
        "You may (should? :-) ) have noticed that Lasso regularization actually forces several weights to be **zero**, not just small.\n",
        "It is one of the mechanisms (in addition to Principal Component Analysis and Singular Value Decomposition that we will see among the unsupervised learning approaches) to identify **which features are least important, or not important at all** for a given target of a given dataset.\n",
        "\n",
        "Credits: the lab was mostly taken from [here](https://medium.com/coinmonks/regularization-of-linear-models-with-sklearn-f88633a93a2)."
      ]
    }
  ]
}